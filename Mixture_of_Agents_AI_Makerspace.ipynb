{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mixture of Agents - AI Makerspace Event\n",
        "\n",
        "In today's notebook we're going to be exploring Together AI's [Mixture of Agents](https://arxiv.org/abs/2406.04692).\n",
        "\n",
        "### What is Mixture-of-Agents?\n",
        "\n",
        "In broad strokes, Mixture-of-Agents (MoA) can be reduced to the following simple idea:\n",
        "\n",
        "> Considering multiple responses when generating a final response tends to lead to a higher quality response.\n",
        "\n",
        "### How does it work?\n",
        "\n",
        "For each MoA system there exist three main components:\n",
        "\n",
        "1. Proposer (or Reference) models - these models propose responses to a user's prompt.\n",
        "2. Aggregators - these models take the proposed outputs, along with a system prompt and the user's prompt to synthesize a response.\n",
        "3. Layers - each MoA system has a number of layers that corresponds to the number of proposal stages and aggregation stages.\n",
        "\n",
        "Let's look at a diagram to better understand layers:\n",
        "\n",
        "\n",
        "![image](https://i.imgur.com/OYRhF8R.png)\n",
        "\n",
        "The basic idea is that each layer will accept some intermediate response from the previous layer, and then output the response set generated to the next layer. This shares similarities with how Deep Learning networks are constructed, albeit at a more \"zoomed out\" level.\n",
        "\n",
        "Let's dive into the notebook to see how this works in practice!"
      ],
      "metadata": {
        "id": "rnEiB3YZZg6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies and API Keys\n",
        "\n",
        "We'll be using Together AI's platform today - which will require an API key to get started!\n",
        "\n",
        "You can follow the process outlined at step 1 [here](https://docs.together.ai/docs/quickstart#1-register-for-an-account) to obtain an API key.\n",
        "\n",
        "> NOTE: This notebook can be executed with the free \\$5 given to new Together AI accounts. This notebook will consume ~$0.01 credits total. Details about pricing are available [here](https://www.together.ai/pricing)."
      ],
      "metadata": {
        "id": "yYQ0dlXXjkb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"TOGETHER_API_KEY\"] = getpass.getpass(\"Together API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve2Qfvl3PnSJ",
        "outputId": "017a512e-8c38-4073-d746-cb0dbef97ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Together API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll install our necessary dependencies."
      ],
      "metadata": {
        "id": "h8nghNTvkfhL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FlPM8xIMwhn",
        "outputId": "c4a1e613-e36c-46f9-eabc-0abf5506c2c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU together openai pyarrow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to run some `asyncio` boilerplate to ensure the notebook can run."
      ],
      "metadata": {
        "id": "WEJyyzbHkhsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "FgrEGRHeWoni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also need to make sure we have set up our Together AI client so that we can interact with the LLMs hosted through their service."
      ],
      "metadata": {
        "id": "6ij5wkpQme-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import os\n",
        "import together\n",
        "from together import AsyncTogether, Together\n",
        "\n",
        "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
        "async_client = AsyncTogether(api_key=os.environ.get(\"TOGETHER_API_KEY\"))"
      ],
      "metadata": {
        "id": "np9Vu-b0RVQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Simple MoA\n",
        "\n",
        "In the following section of the notebook, we'll set up a simple example of the MoA system following Together AI's example from their [GitHub repository](https://github.com/togethercomputer/MoA/blob/main/moa.py).\n",
        "\n",
        "This will be a two layer example - with a proposal layer, and a aggregation layer.\n",
        "\n",
        "Let's check out the diagram:\n",
        "\n",
        "![image](https://i.imgur.com/Ko3Jsil.png)\n",
        "\n",
        "So we'll need to build out the functionality for our MoA system as follows:\n",
        "\n",
        "1. The first layer, which should accept a user prompt, and respond with each Proposal LLM's response to that user prompt.\n",
        "2. A step that concatenates those responses.\n",
        "3. A second layer which augments the system prompt with the outputs of Layer #1 and returns the final Aggregate LLM's response."
      ],
      "metadata": {
        "id": "l__4D3_bklX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proposal Models\n",
        "\n",
        "We can select any number of models here (though cost of each run is directly related to how many models are in each of our layers).\n",
        "\n",
        "You can find a detailed list of available models [here](https://docs.together.ai/docs/chat-models).\n",
        "\n",
        "> NOTE: Keep in mind that, through their research, Together AI noticed that certain models excel in certain roles - which is something to keep in mind when constructing a MoA. Experimentation is encouraged!"
      ],
      "metadata": {
        "id": "1rGqx5WlnNmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "proposal_models = [\n",
        "    \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "    \"google/gemma-2-27b-it\",\n",
        "    \"zero-one-ai/Yi-34B-Chat\",\n",
        "    \"deepseek-ai/deepseek-llm-67b-chat\"\n",
        "]"
      ],
      "metadata": {
        "id": "AeG3MmpEVwX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregator Layer\n",
        "\n",
        "We'll use a moderately powerful model for our Aggregator, given the task of synthesizing a high quality response."
      ],
      "metadata": {
        "id": "1qQuSvgdnruh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aggregator_model = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\""
      ],
      "metadata": {
        "id": "9jgzcmn9WCM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll notice that in the aggregation layer we have a system prompt - this prompt is what provides the instructions to synthesize the previous responses.\n",
        "\n",
        "> NOTE: This prompt is not \"set in stone\" and can, and should, be experimented with - especially depending on the Aggregator used."
      ],
      "metadata": {
        "id": "UzaspW_kn2MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aggreagator_system_prompt = \"\"\"\\\n",
        "You have been provided with a set of responses from various open-source models to the latest user query. \\\n",
        "Your task is to synthesize these responses into a single, high-quality response. \\\n",
        "It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. \\\n",
        "Your response should not simply replicate the given answers but should offer a refined, accurate, and comprehensive reply to the instruction. \\\n",
        "Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.\n",
        "\n",
        "Responses from models:\"\"\""
      ],
      "metadata": {
        "id": "CmAMT8KJWKIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Calling Helper Function\n",
        "\n",
        "We then build a helper function to call the Together API.\n",
        "\n",
        "We'll be using good netiquette and making sure we use exponential backoff for our API calls in the case that we're rate limited.\n",
        "\n",
        "Also - you'll notice that we're providing a few values that could be changed or experimented with:\n",
        "\n",
        "- `temperature` - while the example uses a moderately low temperature, you could experiment with higher or lower to see the overall impact on the final response.\n",
        "- `max_tokens` - we're setting the max response tokens to a reasonable size for this example, though you could try a larger or smaller value to see how it impacted the final response.\n",
        "\n",
        "> NOTE: We are using `run_llm` to run *both* the proposal models and the aggregator model - but it might be prudent or useful to separate the two based on your preferences."
      ],
      "metadata": {
        "id": "VbZ38y2loMlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_llm(model, user_prompt):\n",
        "    \"\"\"Run a single LLM call with a reference model.\"\"\"\n",
        "    for sleep_time in [1, 2, 4]:\n",
        "        try:\n",
        "            response = await async_client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
        "                temperature=0.7,\n",
        "                max_tokens=512,\n",
        "            )\n",
        "            break\n",
        "        except together.error.RateLimitError as e:\n",
        "            print(e)\n",
        "            await asyncio.sleep(sleep_time)\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Ny4vn1DbWMD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constructing MoA\n",
        "\n",
        "Now we can construct our two layer MoA!\n",
        "\n",
        "> NOTE: While we've been using words like \"layers\" - there isn't yet a specific implementation or framework that abstracts the layers in a familiar pattern like PyTorch, or otherwise, so this is more or less just a way to think about the construction of the MoA system for now."
      ],
      "metadata": {
        "id": "IX465XxNoUPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def two_layer_moa(user_prompt):\n",
        "    ### Layer #1: Run each Proposal Model in Layer #1 and collect the results.\n",
        "    results = await asyncio.gather(*[run_llm(model, user_prompt) for model in proposal_models])\n",
        "\n",
        "    ### Intermediate Step: Concatenate the results from Layer #1 to be used as input for Layer #2\n",
        "    concatenated_results = \"\\n\".join([f\"{i+1}. {str(element)}\" for i, element in enumerate(results)])\n",
        "\n",
        "    ### Layer #2: Use the concatenated results as input, provide the aggregator system prompt, and call the aggregator model.\n",
        "    finalStream = client.chat.completions.create(\n",
        "        model=aggregator_model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": aggreagator_system_prompt + \"\\n\" + concatenated_results},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    # Stream the output\n",
        "    for chunk in finalStream:\n",
        "        print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "G1dRYzz9WXKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nothing left to do but run our `two_layer_moa` now!\n",
        "\n",
        "> NOTE: You may see `Error code: 429 ...` this is expected behaviour! We built in a backoff in our `run_llm` helper function to make sure we're using the Together AI API responsibly."
      ],
      "metadata": {
        "id": "3Hpg2dVapJYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"What is the meaning of life?\"\n",
        "\n",
        "asyncio.run(two_layer_moa(user_prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gggclqs9Wcu-",
        "outputId": "0c6f240c-e406-4f1e-be1c-fc63c2465131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error code: 429 - {\"message\": \"You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.\", \"type_\": \"credit_limit\"}\n",
            "Error code: 429 - {\"message\": \"You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.\", \"type_\": \"credit_limit\"}\n",
            "Error code: 429 - {\"message\": \"You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.\", \"type_\": \"credit_limit\"}\n",
            "Error code: 429 - {\"message\": \"You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.\", \"type_\": \"credit_limit\"}\n",
            "The question of the meaning of life is one of the most profound and elusive inquiries humanity has ever pondered. It has been debated by philosophers, theologians, scientists, and scholars across various disciplines for thousands of years. There is no single, definitive answer, as the meaning of life is deeply personal and subjective, shaped by a wide range of beliefs, values, and experiences.\n",
            "\n",
            "Philosophical perspectives offer diverse views on the meaning of life. Existentialism suggests that life has no inherent meaning, and it is up to each individual to create their own purpose through choices and actions. Absurdism posits that the human desire for meaning is in conflict with the universe's apparent indifference. Theism connects the meaning of life to a higher power or God, while hedonism posits that the pursuit of pleasure and avoidance of pain are the ultimate goals.\n",
            "\n",
            "From a scientific and humanistic standpoint, the meaning of life can be viewed through biological and evolutionary lenses, where survival and reproduction are the primary drivers. Humanism, on the other hand, emphasizes the value and agency of human beings, focusing on the pursuit of happiness, fulfillment, and the betterment of the human condition.\n",
            "\n",
            "Ultimately, the search for meaning is an ongoing, personal journey. Many find meaning in self-discovery, personal growth, and the pursuit of happiness and fulfillment. Others find purpose in contributing to society, whether through relationships, careers, or community service. Cultural, social, and historical contexts also shape what individuals believe is important and meaningful.\n",
            "\n",
            "Rather than searching for a definitive answer, it may be more helpful to reflect on personal values, what brings joy and fulfillment, and how one can make a positive impact on the world. The meaning of life is a journey, not a destination, and it can evolve and change over time."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Multi-Layer Proposal MoA\n",
        "\n",
        "Now that we've constructed the simple MoA system, let's extend it to a multi-layer proposal MoA.\n",
        "\n",
        "In essence, this will be the same process - but we will have the ability to include more layers.\n",
        "\n",
        "In the following example - we'll look at constructing the original system as shown here:\n",
        "\n",
        "![image](https://i.imgur.com/OYRhF8R.png)\n",
        "\n",
        "Let's dig right in!"
      ],
      "metadata": {
        "id": "AUTyLFACpoIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intermediate State\n",
        "\n",
        "Due to the fact that we must collect and concatenate responses with the user query between each layer - we'll build a simple helper function to accomplish this.\n",
        "\n",
        "This helper function will take the aggregator system prompt and construct a single useful prompt by concatenating all the other responses together.\n",
        "\n",
        "> NOTE: You might notice that *technically* the aggregation response is being included in each proposal model's prompt past the first layer - and this is correct. However, even though aggregation is *occuring* in Layers #2+, the responses are still only proposals until the final aggregation step occurs."
      ],
      "metadata": {
        "id": "3z4CQX2Rrf_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_responses(system_prompt, results):\n",
        "    \"\"\"Construct a system prompt for layers 2+ that includes the previous responses to synthesize.\"\"\"\n",
        "    return (\n",
        "        system_prompt\n",
        "        + \"\\n\"\n",
        "        + \"\\n\".join([f\"{i+1}. {str(element)}\" for i, element in enumerate(results)])\n",
        "    )"
      ],
      "metadata": {
        "id": "cqsT8RL_XjEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Calling Helper Function\n",
        "\n",
        "Once again, we're going to create a helper function to call our Together AI API hosted models.\n",
        "\n",
        "This time, however, we will need to accomodate the fact that there might be previous responses to consider. If there are, we will leverage the `collect_responses` - if not, we will just use the user prompt."
      ],
      "metadata": {
        "id": "T_YQEjVntIpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_llm(model, user_prompt, prev_response=None):\n",
        "    \"\"\"Run a single LLM call with a model while accounting for previous responses + rate limits.\"\"\"\n",
        "    for sleep_time in [1, 2, 4]:\n",
        "        try:\n",
        "            messages = (\n",
        "                [\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": collect_responses(\n",
        "                            aggreagator_system_prompt, prev_response\n",
        "                        ),\n",
        "                    },\n",
        "                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                ]\n",
        "                if prev_response\n",
        "                else [{\"role\": \"user\", \"content\": user_prompt}]\n",
        "            )\n",
        "            response = await async_client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=messages,\n",
        "                temperature=0.7,\n",
        "                max_tokens=512,\n",
        "            )\n",
        "            print(\"Model: \", model)\n",
        "            break\n",
        "        except together.error.RateLimitError as e:\n",
        "            print(e)\n",
        "            await asyncio.sleep(sleep_time)\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "cTmedi0dXvBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constructing the Multi-Layer Proposal MoA\n",
        "\n",
        "Now we can construct our MoA system!\n",
        "\n",
        "> NOTE: You'll notice that we don't keep a running list of responses, so Layer #3 might be influenced by Layer #1 through the output of Layer #2 - but Layer #3 doesn't directly see the outputs of Layer #1"
      ],
      "metadata": {
        "id": "5FbTHQ4-t3EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def multi_layer_moa(user_prompt, layers):\n",
        "    \"\"\"Run the main loop of the MOA process.\"\"\"\n",
        "    ### Layer #1: In order to collect the first set of results - we run Layer #1 separately\n",
        "    results = await asyncio.gather(*[run_llm(model, user_prompt) for model in proposal_models])\n",
        "\n",
        "    ### Layer #N-1: We run the rest of our layers in a loop, collecting the results for each pass and including them in the next input.\n",
        "    for _ in range(1, layers - 1):\n",
        "        results = await asyncio.gather(\n",
        "            *[run_llm(model, user_prompt, prev_response=results) for model in proposal_models]\n",
        "        )\n",
        "\n",
        "    ### Layer #N: Use the concatenated results as input, provide the aggregator system prompt, and call the aggregator model.\n",
        "    finalStream = client.chat.completions.create(\n",
        "        model=aggregator_model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": collect_responses(aggreagator_system_prompt, results),\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        stream=True,\n",
        "    )\n",
        "    for chunk in finalStream:\n",
        "        print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "UO-2_MZwXzu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asyncio.run(multi_layer_moa(\n",
        "    user_prompt=\"What is the meaning of life?\",\n",
        "    layers=3\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_to32S6FX4F4",
        "outputId": "3d01f09a-46c4-4bdc-b526-319ca254196b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error code: 429 - {\"message\": \"You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.\", \"type_\": \"credit_limit\"}\n",
            "Error code: 429 - {\"message\": \"You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.\", \"type_\": \"credit_limit\"}\n",
            "Error code: 429 - {\"message\": \"You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.\", \"type_\": \"credit_limit\"}\n",
            "Error code: 429 - {\"message\": \"You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.\", \"type_\": \"credit_limit\"}\n",
            "Model:  meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n",
            "Model:  google/gemma-2-27b-it\n",
            "Model:  zero-one-ai/Yi-34B-Chat\n",
            "Model:  deepseek-ai/deepseek-llm-67b-chat\n",
            "Error code: 429 - {\"message\": \"You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.\", \"type_\": \"credit_limit\"}\n",
            "Error code: 429 - {\"message\": \"You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.\", \"type_\": \"credit_limit\"}\n",
            "Error code: 429 - {\"message\": \"You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.\", \"type_\": \"credit_limit\"}\n",
            "Error code: 429 - {\"message\": \"You have been rate limited. Your rate limit is 60 queries per minute. Please navigate to https://api.together.xyz/settings/billing to upgrade to a paid plan.\", \"type_\": \"credit_limit\"}\n",
            "Model:  zero-one-ai/Yi-34B-Chat\n",
            "Model:  meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n",
            "Model:  google/gemma-2-27b-it\n",
            "Model:  deepseek-ai/deepseek-llm-67b-chat\n",
            "The meaning of life is a profound and complex question that has been debated by philosophers, theologians, scientists, and thinkers of all ages and cultures. Ultimately, the answer can vary significantly depending on one's worldview, beliefs, and values. There is no single, universally accepted answer, as the meaning we find is often unique to our individual experiences.\n",
            "\n",
            "Various perspectives offer insights into the meaning of life, including philosophical, religious, scientific, and humanistic views. Existentialists argue that life has no inherent meaning, and it's up to each individual to create their own purpose. Absurdism suggests that life is inherently meaningless, but individuals can find purpose by acknowledging and accepting this absurdity. Hedonism proposes that the meaning of life is to seek pleasure and avoid pain.\n",
            "\n",
            "Religious and spiritual perspectives also offer diverse interpretations. Christianity emphasizes loving God and serving others, while Buddhism seeks enlightenment and escape from the cycle of suffering. Hinduism aims to achieve moksha, or liberation from the cycle of rebirth and suffering.\n",
            "\n",
            "Scientific and humanistic perspectives provide additional insights. Evolutionary theory suggests that the meaning of life is to survive and reproduce, while humanism focuses on seeking knowledge, understanding the universe, and contributing to the betterment of society.\n",
            "\n",
            "Personal reflections also highlight the importance of autonomy, connection, and individual freedom. Some argue that the meaning of life is to be free to make choices and live life on one's own terms, while others believe it's to connect with others, form meaningful relationships, and contribute to the greater good.\n",
            "\n",
            "In conclusion, the meaning of life is a deeply personal and subjective question that each individual must answer for themselves. It's a journey of exploration and discovery, shaped by our experiences, beliefs, and values. Instead of searching for a definitive answer, consider reflecting on what brings you joy, fulfillment, and a sense of purpose. This meaning can evolve over time as you grow and change, guiding you towards a meaningful and fulfilling life that is uniquely your own."
          ]
        }
      ]
    }
  ]
}