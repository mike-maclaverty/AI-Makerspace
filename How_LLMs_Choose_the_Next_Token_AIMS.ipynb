{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How LLMs Choose the Next Token\n",
        "\n",
        "In this notebook we will explore how the end of our LLM (a decoder-only transformer model version) works!\n",
        "\n",
        "More specifically, we'll explore how we go from the decoder stack to a \"next token\"!\n",
        "\n",
        "In order to better understand why we use the loss we do - we'll start here, with generation, to get a sense of what the model is doing \"under the hood\".\n",
        "\n",
        "Let's jump right in!"
      ],
      "metadata": {
        "id": "oBRtW67UDRiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n",
        "\n",
        "Today we'll be using a classic minamalist implementation of a decoder-only transformer model called `nanoGPT`, built by the one-and-only Andrej Karpathy - found [here](https://github.com/karpathy/nanoGPT/tree/master)!\n",
        "\n",
        "It does require a few dependencies - though most are covered by the default Colab environment.\n",
        "\n",
        "> NOTE: You will need to make sure you're in a GPU enabled environment for effective use of this notebook."
      ],
      "metadata": {
        "id": "Lu7EnLapFJYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets tiktoken wandb tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj3ShLeSEvH3",
        "outputId": "bacbedd3-0947-48e4-eae7-9cdb0e0e3df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBdaBurtA8NP",
        "outputId": "0cb68ade-e6ad-46d3-b077-98b488b890c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 671, done.\u001b[K\n",
            "remote: Total 671 (delta 0), reused 0 (delta 0), pack-reused 671\u001b[K\n",
            "Receiving objects: 100% (671/671), 947.92 KiB | 2.49 MiB/s, done.\n",
            "Resolving deltas: 100% (379/379), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xjcjW0zEhbM",
        "outputId": "28928997-7944-4382-b337-f8db768887c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Tokens!\n",
        "\n",
        "Let's just try to do some inference and see what happens before we dig in."
      ],
      "metadata": {
        "id": "6ZtIRopJGfRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSKkqMHdErid",
        "outputId": "51e08eec-d4f6-42f0-df36-7311e983adac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "One possibility is that they have a universe-wide perspective that allows for no contradictions. And, if so, then why is there contradiction?\n",
            "\n",
            "Another possibility is that the universe is one big joke. They are not the only joke in the universe. But it is not an empty universe.\n",
            "\n",
            "Hence the problems with the universe's Big Bang theory, which tells us that the universe started out in a singularity with a zero initial mass. The Big Bang is essentially consistent with\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll notice that we pass *in* text - and we receive *back* text from our model."
      ],
      "metadata": {
        "id": "i9VQDtfcGych"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Does the LLM Generate Tokens\n",
        "\n",
        "So, we pas in text - and get text back - but how do we actually generate each token?\n",
        "\n",
        "You might have heard the term \"auto-regressive\" or \"causal\" kicking around when reading about LLMs - and what those terms, in a simplified sense, mean is straightfoward enough:\n",
        "\n",
        "- They take an input, and generate a single token\n",
        "- They append that token to the input and repeat this process for as long as we want it to repeat (or use heuristics to determine when to stop, such as when we see a stop token)\n",
        "\n",
        "Let's take a look at the function that does this in the `nanoGPT` repository.\n",
        "\n"
      ],
      "metadata": {
        "id": "NKNzRbwuG0M2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "@torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "```"
      ],
      "metadata": {
        "id": "2Jpj1ytjH5--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is a Logit?!\n",
        "\n",
        "Technically - a logit is a \"raw unnormalized score\".\n",
        "\n",
        "However, we can think of them as scores for each token in our vocabulary. These scores aren't probabilities in and of themselves - but they can be easily converted to probabilities through the softmax function."
      ],
      "metadata": {
        "id": "tvDfy32YKnxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Temperature Doing?!\n",
        "\n",
        "While something like `top_k` makes intuitive sense - what in the heck is temperature doing here?\n",
        "\n",
        "In order to understand - let's look at a few examples!\n",
        "\n",
        "Starting with an easy `temperature = 1.0`.\n",
        "\n",
        "> NOTE: We'll also define our softmax function!"
      ],
      "metadata": {
        "id": "sRvnaByHLUr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum())"
      ],
      "metadata": {
        "id": "lOWCrlUsMSnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "temperature = 1.0\n",
        "\n",
        "logits = np.array([6, 2, 7, 0.1, -8, 9])\n",
        "\n",
        "temp_scaled_logits = logits / temperature\n",
        "print(f\"Scaled Logits: {temp_scaled_logits}\")\n",
        "\n",
        "softmaxed_logits = softmax(temp_scaled_logits)\n",
        "print(f\"Softmax-ed Logits: {softmaxed_logits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXpso0sYLdwI",
        "outputId": "c25d26dd-c897-4bab-e6e7-87b36fa043b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Logits: [ 6.   2.   7.   0.1 -8.   9. ]\n",
            "Softmax-ed Logits: [4.19729385e-02 7.68761185e-04 1.14094276e-01 1.14982549e-04\n",
            " 3.49017038e-08 8.43049007e-01]\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see - our logits are not changed, and our softmax output has quite a bit of variety - from `e-08` to `e-1`, meaning that our index with the score `9` is most likely to be selected, but it's not absurdly likely.\n",
        "\n",
        "Let's look at an example with a very low temperature next!"
      ],
      "metadata": {
        "id": "AbE3L_VKMByY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 0.1\n",
        "\n",
        "logits = np.array([6, 2, 7, 0.1, -8, 9])\n",
        "\n",
        "temp_scaled_logits = logits / temperature\n",
        "print(f\"Scaled Logits: {temp_scaled_logits}\")\n",
        "\n",
        "softmaxed_logits = softmax(temp_scaled_logits)\n",
        "print(f\"Softmax-ed Logits: {softmaxed_logits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE5fO2SSM3Ct",
        "outputId": "ee70baaf-104f-417e-e232-a9338df0804c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Logits: [ 60.  20.  70.   1. -80.  90.]\n",
            "Softmax-ed Logits: [9.35762295e-14 3.97544973e-31 2.06115362e-09 2.22736356e-39\n",
            " 1.47889750e-74 9.99999998e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see - now that we changed our temperature to be very low - the index with score `9` is *vastly* more likely than any other option.\n",
        "\n",
        "This is the idea that a low (<1) temperature value will scale our logits to be larger - resulting in a sharper probability distribution after softmax.\n",
        "\n",
        "Let's look at a final example with a higher temperature."
      ],
      "metadata": {
        "id": "5VVVtg8_NQ06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 10\n",
        "\n",
        "logits = np.array([6, 2, 7, 0.1, -8, 9])\n",
        "\n",
        "temp_scaled_logits = logits / temperature\n",
        "print(f\"Scaled Logits: {temp_scaled_logits}\")\n",
        "\n",
        "softmaxed_logits = softmax(temp_scaled_logits)\n",
        "print(f\"Softmax-ed Logits: {softmaxed_logits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-tRO3UDNmt1",
        "outputId": "558bc415-8854-4e70-d589-b0dbddb85b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Logits: [ 0.6   0.2   0.7   0.01 -0.8   0.9 ]\n",
            "Softmax-ed Logits: [0.20299317 0.13607039 0.22434215 0.11252466 0.0500575  0.27401212]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see that, while our index with score `9` is still the most likely - we can see that the probabilities are much closer together!"
      ],
      "metadata": {
        "id": "CBMM9M1VM7We"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Psuedo-Code For Generation\n",
        "\n",
        "Now that we have an intuition for what logits and temperature are doing - let's see what our generation code is doing in simpler terms:\n",
        "\n",
        "1. For some range (user decided)\n",
        "2. We check and make sure our current range of indices will fit in our block size - if they don't, we trim it so it does\n",
        "3. We get the logits for the provided indices\n",
        "4. We scale the logits by the user defined temperature (the default is 1)\n",
        "5. We optionally crop the logits by our tok k - meaning we only keep the top k values in our logits. This effectively limits our choices to only the k most likely tokens\n",
        "6. We apply softmax to convert our logits into a probability distribution\n",
        "7. We sample from that probability\n",
        "8. We add the sampled index to our input indices (auto-regressive, anyone?)\n",
        "9. We're done!"
      ],
      "metadata": {
        "id": "BMMwZ5AFIvvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How do we get to Logits?\n",
        "\n",
        "There's a question that might be nagging at you - how do we get from our decoder stack output to a series of scores for each token in our vocabulary?\n",
        "\n",
        "That's where our `lm_head` comes in - in this case, a linear layer!\n",
        "\n",
        "Let's take a look at this layer as it's define in `nanoGPT`."
      ],
      "metadata": {
        "id": "m13Dcv6AOCSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "```"
      ],
      "metadata": {
        "id": "od2b06kXOiK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the `lm_head` is a linear layer that has input size `n_embed` (the internal dimension of our model), and output size `vocab_size` (the vocabulary size).\n",
        "\n",
        "So what this means, is that the `lm_head` is a linear projection across the vocabulary - which is what ultimately provides our logits (or scores) as determined by the multiple-decoder stacks that we have passed our inputs through.\n",
        "\n",
        "Essentially, this linear layer acts as a translation between our model's internal representation and our desired output format which, in this case, is tokens!"
      ],
      "metadata": {
        "id": "7RbnvzIUOk8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logits and Loss\n",
        "\n",
        "Okay, so now we have a better understanding of how a model generates the next token:\n",
        "\n",
        "1. The decoder-blocks take our input and compute attention scores\n",
        "2. We project those scores from our internal model dimension onto our vocabulary\n",
        "3. We use the obtained \"raw unnormalized scores\" (logits!) to find a probability distribution (through softmax, after some potential processing)\n",
        "4. We sample from that probability distribution to find our next token!\n",
        "5. We append the token to our input\n",
        "6. RINSE AND REPEAT\n",
        "\n",
        "So - how does this relate to loss?\n",
        "\n",
        "Well - there's a question lurking in there, which is:\n",
        "\n",
        "\"How do we know that the scores we assigned are what they're supposed to be? Or even close?\"\n",
        "\n",
        "For that - we'll need loss!"
      ],
      "metadata": {
        "id": "Obc1fMElPsMc"
      }
    }
  ]
}