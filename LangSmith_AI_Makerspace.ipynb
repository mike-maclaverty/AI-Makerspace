{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangSmith Overview with AI Makerspace\n",
        "\n",
        "Today we'll be looking at an amazing tool:\n",
        "\n",
        "[LangSmith](https://docs.smith.langchain.com/)!\n",
        "\n",
        "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n"
      ],
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Depenedencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for a simple RAG system built on top of LangChain's blogs!"
      ],
      "metadata": {
        "id": "tw5ok9p-XuUs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhSjB1O6-Y0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f3812f6-4d9b-4f42-e466-071a1021c4ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain_core langchain_openai langchain_community langsmith openai tiktoken cohere -qU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADl8-whIAUHD",
        "outputId": "f3589eca-3b7d-431c-eb30-7d31461a6478"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic RAG Chain\n",
        "\n",
        "Now we'll set up our basic RAG chain, first up we need a model!"
      ],
      "metadata": {
        "id": "T_NpPwk1YAgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI Model\n",
        "\n",
        "\n",
        "We'll use OpenAI's `gpt-3.5-turbo` model to ensure we can use a stronger model for decent evaluation later!\n",
        "\n",
        "Notice that we can tag our resources - this will help us be able to keep track of which resources were used where later on!"
      ],
      "metadata": {
        "id": "CUWXhsNVYLTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "base_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", tags=[\"base_llm\"])"
      ],
      "metadata": {
        "id": "CSgK6jgw_tI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Asyncio Bug Handling\n",
        "\n",
        "This is necessary for Colab."
      ],
      "metadata": {
        "id": "iiagvgVDYTPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "ntIqnv4cA5gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SiteMap Loader\n",
        "\n",
        "We'll use a SiteMapLoader to scrape the LangChain blogs."
      ],
      "metadata": {
        "id": "PDO0XJqbYabb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import SitemapLoader\n",
        "\n",
        "documents = SitemapLoader(web_path=\"https://blog.langchain.dev/sitemap-posts.xml\").load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAS3QBQSARiw",
        "outputId": "cceb7acc-dd77-40cc-cd3f-acbdb626dbd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching pages: 100%|##########| 213/213 [00:08<00:00, 24.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].metadata[\"source\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_s_x87H0BYmn",
        "outputId": "ff8fd85b-a5b8-453f-a208-a2b0a8aa0fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://blog.langchain.dev/documentation-refresh-for-langchain-v0-2/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RecursiveCharacterTextSplitter\n",
        "\n",
        "We're going to use a relatively naive text splitting strategy today!"
      ],
      "metadata": {
        "id": "F79PdFcaYfBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "split_documents = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 20\n",
        ").split_documents(documents)"
      ],
      "metadata": {
        "id": "NmCdYTTTA4du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(split_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLA5-LNBBVM-",
        "outputId": "3a1b3ac2-a00c-4f5d-fdc7-ba3de68aee22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1498"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings\n",
        "\n",
        "We'll be leveraging OpenAI's [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings/how-to-get-embeddings) today!"
      ],
      "metadata": {
        "id": "EUsEc07iYnwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "base_embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "QVhMN0aaBrsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FAISS VectorStore Retriever\n",
        "\n",
        "Now we can use a FAISS VectorStore to embed and store our documents and then convert it to a retriever so it can be used in our chain!"
      ],
      "metadata": {
        "id": "NLoO_2MaY0TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoZFgJB-CAdS",
        "outputId": "0ca564fc-0294-49b8-f677-c648cde8fb50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(split_documents, base_embeddings_model)"
      ],
      "metadata": {
        "id": "nBTK9kSFBWM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "ZpwDxlniCJRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template\n",
        "\n",
        "All we have left is a prompt template, which we'll create here!"
      ],
      "metadata": {
        "id": "U2GPhHPAY5yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "base_rag_prompt_template = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know the answer based on the context, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "base_rag_prompt = ChatPromptTemplate.from_template(base_rag_prompt_template)"
      ],
      "metadata": {
        "id": "YAU74penCNmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LCEL Chain\n",
        "\n",
        "Now that we have:\n",
        "\n",
        "- Embeddings Model\n",
        "- Generation Model\n",
        "- Retriever\n",
        "- Prompt\n",
        "\n",
        "We're ready to build our LCEL chain!\n",
        "\n",
        "Keep in mind that we're returning our source documents with our queries - while this isn't necessary, it's a great thing to get into the habit of doing."
      ],
      "metadata": {
        "id": "xmT5VyLmZAAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "base_rag_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": base_rag_prompt | base_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "pqVAsUc_Cp-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "8fNjMoS-ZVo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is a good way to evaluate agents?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Dq9rCScDfBE",
        "outputId": "09444354-f4bf-4c4d-b0ce-a222d3a62e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'response': 'A good way to evaluate agents is by testing their capabilities in various tasks that are prerequisites for common agentic workflows, such as planning, task decomposition, function calling, and the ability to override pre-trained biases when needed. Additionally, measuring overall performance across all tasks and analyzing specific key findings can help assess the effectiveness of agents.',\n",
              " 'context': [Document(page_content=\"that there are some downsides/dangers:With agents, they can occasionally spiral out of control. That's why we've added controls to our AgentExecutor to cap them at a certain max amount of steps. It's also worth noting that this is a VERY focused agent, in that it's only given one tool (and a pretty simple tool at that). In general, the fewer (and simpler) tools an agent is given, the more likely it is to be reliable.By remembering ai <-> tool interactions, that can hog the context window occasionally. That's why we've included a flag to disable that type of memory, and more generally have made memory pretty plug-and-play.This new agent is in both Python and JS - you can use these guides to get started:JSPythonLLM applications are rapidly evolving. Our NotionQA demo was one of the first we did - and although it was only ~9 months ago the best practices have shifted dramatically since then. This currently represents our best guess at what a GenAI question-answering system should look like, combining the grounded-ness of RAG with the UX of chat and the flexibility of agents.We've got a few more ideas on how this can be further improved - we'll be rolling those out over the next few weeks. As always, we'd love to hear from you with any suggestions or ideas.\", metadata={'source': 'https://blog.langchain.dev/conversational-retrieval-agents/', 'loc': 'https://blog.langchain.dev/conversational-retrieval-agents/', 'lastmod': '2023-08-18T21:52:40.000Z'}),\n",
              "  Document(page_content=\"Agents may be the ‚Äúkiller‚Äù LLM app, but building and evaluating agents is hard. Function calling is a key skill for effective tool use, but there aren‚Äôt many good benchmarks for measuring function calling performance. Today, we are excited to release four new test environments for benchmarking LLMs‚Äô ability to effectively use tools to accomplish tasks. We hope this makes it easier for everyone to test different LLM and prompting strategies to show what enables the best agentic behavior.Example successful tool use for the Relational Data taskWe designed these tasks to test capabilities we consider to be prerequisites for common agentic workflows, such as planning / task decomposition, function calling, and the ability to override pre-trained biases when needed. If an LLM is unable to solve these types of tasks (without explicit fine-tuning), it will likely struggle to perform and generalize reliably for other workflows where ‚Äúreasoning‚Äù is required. Below are some key take-ways for those eager to see our findings:Overall performance across all tasks (weighted average). Error bars computed using standard error.üí°Key Findings - All of the models can fail over longer trajectories, even for simple tasks.- GPT-4 got the highest score on the Relational Data task, which most closely approximates common usage.- GPT-4 seems to be worse than GPT-3.5 on the Multiverse Math task; it's possible its pretrained bias hinders its performance in an example of inverse scaling.- Claude-2.1 performs within the error bounds of GPT-4 for 3 of 4 tasks, though seems to lag GPT-4 on the relational data task.- Despite outputting well-formatted tool invocations, AnyScale‚Äôs fine-tuned variant of Mistral 7b struggles to reliably compose more than 2 calls. Future open-source function calling efforts should focus on function composition in addition to single-call correctness.- In addition to model quality, service reliability is important. We ran into frequent random 5xx errors from the most popular model providers.ü¶úSo what?- Superhuman model knowledge doesn't help if your task or knowledge differs significantly from its pre-training. Validate the LLM you choose on the behavior patterns you need it to excel on before deploying.- Planning is still hard for LLMs - the likelihood of failure increases with the number of required steps,\", metadata={'source': 'https://blog.langchain.dev/benchmarking-agent-tool-use/', 'loc': 'https://blog.langchain.dev/benchmarking-agent-tool-use/', 'lastmod': '2023-12-20T17:54:03.000Z'}),\n",
              "  Document(page_content='dozens of tools.Separate prompts can give better results. Each prompt can have its own instructions and few-shot examples. Each agent could even be powered by a separate fine-tuned LLM!Helpful conceptual model to develop. You can evaluate and improve each agent individually without breaking the larger application. Multi-agent designs allow you to divide complicated problems into tractable units of work that can be targeted by specialized agents and LLM programs.Multi-agent examplesWe\\'ve added three separate example of multi-agent workflows to the langgraph repo. Each of these has slightly different answers for the above two questions, which we will go over when we highlight the examples. It\\'s important to note that these three examples are only a few of the possible examples we could highlight - there are almost assuredly other examples out there and we look forward to seeing what the community comes up with!Multi Agent CollaborationCode links:PythonJSIn this example, the different agents collaborate on a shared scratchpad of messages. This means that all the work either of them do is visible to the other. This has the benefit that other agents can see all the individual steps done. This has the downside that sometimes is it overly verbose and unnecessary to pass ALL this information along, and sometimes only the final answer from an agent is needed. We call this collaboration because of the shared nature the scratchpad.What are the multiple independent agents?In this case, the independent agents are actually just a single LLM call. Specifically, they are a specific prompt template (to format inputs in a specific way with a specific system message) plus an LLM call.How are those agents connected?Here is a visualization of how these agents are connected:The main thing controlling the state transitions is the router, but it is a rule-based router and so is rather quite simple. Basically, after each LLM call it looks at the output. If a tool is invoked, then it calls that tool. If no tool is called and the LLM responds \"FINAL ANSWER\" then it returns to the user. Otherwise (if no tool is called and the LLM does not respond \"FINAL ANSWER\") then it goes to the other LLM.Agent SupervisorExamples:PythonJSIn this example, multiple agents are connected, but compared to above they do NOT share a shared scratchpad. Rather, they have their own independent scratchpads, and then their final responses are appended', metadata={'source': 'https://blog.langchain.dev/langgraph-multi-agent-workflows/', 'loc': 'https://blog.langchain.dev/langgraph-multi-agent-workflows/', 'lastmod': '2024-01-23T17:32:37.000Z'}),\n",
              "  Document(page_content='or return a response if the agent is done.The LLMSingleActionAgent is initialized with the prompt template shown below, which is ultimately used in the plan method to get an AgentAction. Let‚Äôs dive into this prompt a bit:The first thing to note is the framing: you are a character playing in front of a live audience. We found that directly telling the agent this generated a lot more chatter, which helped make our simulation more interesting to observers. It inspired the agents to talk to themselves when they were doing a task (for example, saying things like ‚ÄúWow that‚Äôs interesting‚Äù when searching on google).The next thing to note is the prioritization of dialog. This is something we did in the react function as well. Even if the current plan has nothing to do with dialog, we instruct the agent here to finish their pending conversations first, reminding them they need not respond all the time.The rest of the prompt is pretty straight forward, and heavily inspired by the default langchain agent prompt. We include the agent‚Äôs bio, their location context, relevant memories, conversation history, and tools available. Quick note: every agent has a public bio and a private bio. The public bio is a description that all the other agents have access to, such as their role, their appearence, and their name. A private bio includes details that only the agent themselves know, such as their insecurities and desires.We referred to this plan + tool usage action as a single step. The execute function of the PlanExecutor class runs the agent for one step each time it‚Äôs called. After a step is run, a string representation of the step is added to a list of historical steps contained within the current plan (we borrow from Langchain‚Äôs terminology here when we call that a scratchpad).We save the scratchpad at the end of the function so that the agent can pick up where it left off in the next run of the agent loop.Agent.reflectAs the final step of every agent loop, we check if it‚Äôs time to reflect. This function is triggered every time the total importance score across all of an agent‚Äôs memories reaches a multiple of 100. This way, if the agent is experiencing something extremely exciting and memorable, it will reflect more often, and vice versa.This reflection logic was borrowed directly from the brilliant authors of the ‚ÄúGenerative Agents‚Äù', metadata={'source': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'loc': 'https://blog.langchain.dev/gpteam-a-multi-agent-simulation/', 'lastmod': '2023-06-24T21:34:28.000Z'})]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangSmith\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ],
      "metadata": {
        "id": "fJtSdDsXZXam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangSmith - {unique_id}\""
      ],
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangSmith API\n",
        "\n",
        "In order to use LangSmith - you will need a beta key, you can join the queue through the `Beta Sign Up` button on LangSmith's homepage!\n",
        "\n",
        "Join [here](https://www.langchain.com/langsmith)"
      ],
      "metadata": {
        "id": "Ms4msyKLaIr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVq1EYngEMhV",
        "outputId": "edcbbc23-83c9-4b2c-e914-fe44f13b2db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test our our first generation!"
      ],
      "metadata": {
        "id": "6qy0MMBLacXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_rag_chain.invoke({\"question\" : \"What is LangSmith?\"}, {\"tags\" : [\"Demo Run\"]})['response']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "3eoqBtBQERXP",
        "outputId": "a12f95ae-7485-40e9-f30b-33f42c391cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangSmith is a unified platform for debugging, testing, evaluating, and monitoring LLM (Large Language Models) applications. It provides features such as utilizing existing datasets, creating new datasets, running them against chains, visual feedback on outputs, accuracy metrics, evaluation of LLM runs, monitoring AI processes, and more.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Testing Dataset\n",
        "\n",
        "Now we can create a dataset using some user defined questions, and providing the retrieved context as a \"ground truth\" context.\n",
        "\n",
        "> NOTE: There are many different ways you can approach this specific task - generating ground truth answers with AI, using human experts to generate golden datasets, and more!"
      ],
      "metadata": {
        "id": "fLxh0-thanXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "test_inputs = [\n",
        "    \"What is LangSmith?\",\n",
        "    \"What is LangServe?\",\n",
        "    \"How could I benchmark RAG on tables?\",\n",
        "    \"What was exciting about LangChain's first birthday?\",\n",
        "    \"What features were released for LangChain on August 7th?\",\n",
        "    \"What is a conversational retrieval agent?\"\n",
        "]\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"langsmith-demo-dataset-v1\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"LangChain Blog Test Questions\"\n",
        ")\n",
        "\n",
        "for input in test_inputs:\n",
        "  client.create_example(\n",
        "      inputs={\"question\" : input},\n",
        "      outputs={\"answer\" : base_rag_chain.invoke({\"question\" : input})[\"context\"]},\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ],
      "metadata": {
        "id": "T9exE2e6F3gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "Now we can run the evaluation!"
      ],
      "metadata": {
        "id": "QXgi14vSbFIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "  evaluators=[\n",
        "    RunEvalConfig.CoTQA(llm=eval_llm, prediction_key=\"response\"),\n",
        "    RunEvalConfig.Criteria(\"harmfulness\", prediction_key=\"response\"),\n",
        "  ]\n",
        ")\n",
        "\n",
        "base_rag_base_run = run_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=base_rag_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CENtd4K_IQa3",
        "outputId": "b9eda203-b5cb-4c62-a4c0-8b03c6cf4693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'whispered-side-96' at:\n",
            "https://smith.langchain.com/o/340cd80b-3296-5752-9a9e-58582118073a/datasets/61bdc417-2079-44d4-916c-d9b553fa6f32/compare?selectedSessions=318afa52-d514-4ccb-8df8-84844d1760bb\n",
            "\n",
            "View all tests for Dataset langsmith-demo-dataset-v1 at:\n",
            "https://smith.langchain.com/o/340cd80b-3296-5752-9a9e-58582118073a/datasets/61bdc417-2079-44d4-916c-d9b553fa6f32\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.Contextual Accuracy  feedback.harmfulness error  execution_time                                run_id\n",
            "count                           6.00                  6.00     0            6.00                                     6\n",
            "unique                           NaN                   NaN     0             NaN                                     6\n",
            "top                              NaN                   NaN   NaN             NaN  119add34-9617-4904-bf33-71f75c40c697\n",
            "freq                             NaN                   NaN   NaN             NaN                                     1\n",
            "mean                            0.83                  0.00   NaN            2.15                                   NaN\n",
            "std                             0.41                  0.00   NaN            0.33                                   NaN\n",
            "min                             0.00                  0.00   NaN            1.80                                   NaN\n",
            "25%                             1.00                  0.00   NaN            1.89                                   NaN\n",
            "50%                             1.00                  0.00   NaN            2.15                                   NaN\n",
            "75%                             1.00                  0.00   NaN            2.28                                   NaN\n",
            "max                             1.00                  0.00   NaN            2.67                                   NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Reranking\n",
        "\n",
        "We'll add reranking to our RAG application to confirm the claim made by [Cohere](https://cohere.com/rerank)!\n",
        "\n",
        "`Improve search performance with a single line of code`\n",
        "\n",
        "We'll put that to the test today!"
      ],
      "metadata": {
        "id": "fOB0u8-RemQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIgP810vPGdi",
        "outputId": "65deda1d-49d3-4579-cf27-fa6e95e44c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Cohere API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_retriever_expander = vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\" : 10}\n",
        ")"
      ],
      "metadata": {
        "id": "Z3VTNGXlO-m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank\n",
        "\n",
        "reranker = CohereRerank()\n",
        "rerank_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker, base_retriever=base_retriever_expander\n",
        ")"
      ],
      "metadata": {
        "id": "Uk7EPsa3PiUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0686c96-9551-422a-dd71-3896639abc00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 0.3.0. An updated version of the class exists in the langchain-cohere package and should be used instead. To use it run `pip install -U langchain-cohere` and import as `from langchain_cohere import CohereRerank`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recreating our Chain with Reranker\n",
        "\n",
        "Now we can recreate our chain using the reranker."
      ],
      "metadata": {
        "id": "MBN0h0Zbe7up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rerank_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | rerank_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": base_rag_prompt | base_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n",
        ")\n",
        "\n",
        "rerank_rag_chain = rerank_rag_chain.with_config({\"tags\" : [\"cohere-rerank\"]})"
      ],
      "metadata": {
        "id": "kfckjK3QPqhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improved Evaluation\n",
        "\n",
        "Now we can leverage the full suite of LangSmith's evaluation to evaluate our chains on multiple metrics, including custom metrics!"
      ],
      "metadata": {
        "id": "6qEHGMLAfISS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_config = RunEvalConfig(\n",
        "  evaluators=[\n",
        "    RunEvalConfig.CoTQA(llm=eval_llm, prediction_key=\"response\"),\n",
        "    RunEvalConfig.Criteria(\"harmfulness\", prediction_key=\"response\"),\n",
        "    RunEvalConfig.LabeledCriteria(\n",
        "        {\n",
        "            \"helpfulness\" : (\n",
        "                \"Is this submission helpful to the user,\"\n",
        "                \"taking into account the correct reference answer?\"\n",
        "            )\n",
        "        },\n",
        "        prediction_key=\"response\"\n",
        "    ),\n",
        "    RunEvalConfig.LabeledCriteria(\n",
        "        {\n",
        "            \"litness\" : (\n",
        "                \"Is this submission lit, dope, or cool?\"\n",
        "            )\n",
        "        },\n",
        "        prediction_key=\"response\"\n",
        "    ),\n",
        "    RunEvalConfig.LabeledCriteria(\"conciseness\", prediction_key=\"response\"),\n",
        "    RunEvalConfig.LabeledCriteria(\"coherence\", prediction_key=\"response\"),\n",
        "    RunEvalConfig.LabeledCriteria(\"relevance\", prediction_key=\"response\")\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "nQQXpFg2SV5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Eval on Each Chain\n",
        "\n",
        "Now we can evaluate each of our chains!"
      ],
      "metadata": {
        "id": "DPxlhXLmft0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_chain_results = run_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=base_rag_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuVnTrncU9LK",
        "outputId": "27b8d320-2927-40c1-c3b2-1aa02ad0a60c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'stupendous-test-17' at:\n",
            "https://smith.langchain.com/o/340cd80b-3296-5752-9a9e-58582118073a/datasets/61bdc417-2079-44d4-916c-d9b553fa6f32/compare?selectedSessions=ba5a6170-232f-4e8b-9dab-f3a19b5b8e5e\n",
            "\n",
            "View all tests for Dataset langsmith-demo-dataset-v1 at:\n",
            "https://smith.langchain.com/o/340cd80b-3296-5752-9a9e-58582118073a/datasets/61bdc417-2079-44d4-916c-d9b553fa6f32\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.Contextual Accuracy  feedback.harmfulness  feedback.helpfulness  feedback.litness  feedback.conciseness  feedback.coherence  feedback.relevance error  execution_time                                run_id\n",
            "count                           6.00                  6.00                  6.00              6.00                  6.00                6.00                6.00     0            6.00                                     6\n",
            "unique                           NaN                   NaN                   NaN               NaN                   NaN                 NaN                 NaN     0             NaN                                     6\n",
            "top                              NaN                   NaN                   NaN               NaN                   NaN                 NaN                 NaN   NaN             NaN  120c8724-0112-4077-9782-cebb2961a360\n",
            "freq                             NaN                   NaN                   NaN               NaN                   NaN                 NaN                 NaN   NaN             NaN                                     1\n",
            "mean                            1.00                  0.00                  0.67              0.83                  0.67                1.00                0.67   NaN            1.96                                   NaN\n",
            "std                             0.00                  0.00                  0.52              0.41                  0.52                0.00                0.52   NaN            0.60                                   NaN\n",
            "min                             1.00                  0.00                  0.00              0.00                  0.00                1.00                0.00   NaN            1.26                                   NaN\n",
            "25%                             1.00                  0.00                  0.25              1.00                  0.25                1.00                0.25   NaN            1.62                                   NaN\n",
            "50%                             1.00                  0.00                  1.00              1.00                  1.00                1.00                1.00   NaN            1.76                                   NaN\n",
            "75%                             1.00                  0.00                  1.00              1.00                  1.00                1.00                1.00   NaN            2.48                                   NaN\n",
            "max                             1.00                  0.00                  1.00              1.00                  1.00                1.00                1.00   NaN            2.72                                   NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rerank_chain_results = run_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=rerank_rag_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS0m4cunQ1m1",
        "outputId": "a873c378-9669-4c5b-add0-e9e4b84ea475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'left-measure-57' at:\n",
            "https://smith.langchain.com/o/340cd80b-3296-5752-9a9e-58582118073a/datasets/61bdc417-2079-44d4-916c-d9b553fa6f32/compare?selectedSessions=f39cb22d-1974-438d-ae18-6d63fede9745\n",
            "\n",
            "View all tests for Dataset langsmith-demo-dataset-v1 at:\n",
            "https://smith.langchain.com/o/340cd80b-3296-5752-9a9e-58582118073a/datasets/61bdc417-2079-44d4-916c-d9b553fa6f32\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.Contextual Accuracy  feedback.harmfulness  feedback.helpfulness  feedback.litness  feedback.conciseness  feedback.coherence  feedback.relevance error  execution_time                                run_id\n",
            "count                           6.00                  6.00                  6.00              6.00                  6.00                6.00                6.00     0            6.00                                     6\n",
            "unique                           NaN                   NaN                   NaN               NaN                   NaN                 NaN                 NaN     0             NaN                                     6\n",
            "top                              NaN                   NaN                   NaN               NaN                   NaN                 NaN                 NaN   NaN             NaN  bd3ec139-1e1e-4178-8830-366295828d0e\n",
            "freq                             NaN                   NaN                   NaN               NaN                   NaN                 NaN                 NaN   NaN             NaN                                     1\n",
            "mean                            1.00                  0.00                  0.83              0.67                  0.33                1.00                0.33   NaN            2.29                                   NaN\n",
            "std                             0.00                  0.00                  0.41              0.52                  0.52                0.00                0.52   NaN            0.52                                   NaN\n",
            "min                             1.00                  0.00                  0.00              0.00                  0.00                1.00                0.00   NaN            1.68                                   NaN\n",
            "25%                             1.00                  0.00                  1.00              0.25                  0.00                1.00                0.00   NaN            1.88                                   NaN\n",
            "50%                             1.00                  0.00                  1.00              1.00                  0.00                1.00                0.00   NaN            2.28                                   NaN\n",
            "75%                             1.00                  0.00                  1.00              1.00                  0.75                1.00                0.75   NaN            2.75                                   NaN\n",
            "max                             1.00                  0.00                  1.00              1.00                  1.00                1.00                1.00   NaN            2.84                                   NaN\n"
          ]
        }
      ]
    }
  ]
}