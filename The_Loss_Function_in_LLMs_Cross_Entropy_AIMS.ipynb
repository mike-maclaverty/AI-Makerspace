{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "collapsed_sections": [
        "otLV55h9T322"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Loss Function in LLMs - Cross Entropy- AIMS\n",
        "\n",
        "Now that we have a better understanding of what decoder-only transformer based LLMs are doing to predict the next token - let's examine this through `nanoGPT`, built by the one-and-only Andrej Karpathy - found [here](https://github.com/karpathy/nanoGPT/tree/master)!"
      ],
      "metadata": {
        "id": "4QBs8FHvShu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n",
        "\n",
        "We'll start by loading the repository and the requirements not included in Colab by default."
      ],
      "metadata": {
        "id": "-Qtr6wnLTYlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets tiktoken wandb tqdm"
      ],
      "metadata": {
        "id": "XcfnV1C3Cz50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n9tJuC0BKFJ",
        "outputId": "1d548c72-59f8-45e5-fb50-48a9be24b88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nanoGPT' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucLa1gp3CmB0",
        "outputId": "771ea959-fd73-4616-b381-043f02f4d57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "In order to have the correct form of our inputs (tokens) we need to prepare our dataset - let's do this using the script provided by the repository!"
      ],
      "metadata": {
        "id": "kstygtQwVOUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/shakespeare/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNV0Z2uLVVay",
        "outputId": "196babaa-5d28-4446-9fed-b7515befbee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop\n",
        "\n",
        "We'll leverage the training loop from Karpathy's repository to focus in on the specifics of where we're using loss - how we're using it - and what it means!"
      ],
      "metadata": {
        "id": "hPyDaUl3TgSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preamble Code"
      ],
      "metadata": {
        "id": "otLV55h9T322"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
        "# I/O\n",
        "out_dir = 'out'\n",
        "eval_interval = 2000\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "# wandb logging\n",
        "wandb_log = False # disabled by default\n",
        "wandb_project = 'owt'\n",
        "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
        "# data\n",
        "dataset = 'shakespeare'\n",
        "gradient_accumulation_steps = 1 # used to simulate larger batch sizes\n",
        "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 1024\n",
        "# model\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4 # max learning rate\n",
        "max_iters = 10 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 10 # how many steps to warm up for\n",
        "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# poor man's data loader\n",
        "data_dir = os.path.join('data', dataset)\n",
        "def get_batch(split):\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# attempt to derive vocab_size from the dataset\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = None\n",
        "if os.path.exists(meta_path):\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = meta['vocab_size']\n",
        "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
        "\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
        "if init_from == 'scratch':\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "elif init_from == 'resume':\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "    # resume training from a checkpoint.\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    # force these config attributes to be equal otherwise we can't even resume training\n",
        "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    # create the model\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    # fix the keys of the state dictionary :(\n",
        "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint['iter_num']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "elif init_from.startswith('gpt2'):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "    # initialize from OpenAI GPT-2 weights\n",
        "    override_args = dict(dropout=dropout)\n",
        "    model = GPT.from_pretrained(init_from, override_args)\n",
        "    # read off the created config params, so we can store them into checkpoint correctly\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = getattr(model.config, k)\n",
        "# crop down the model block size if desired, using model surgery\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None # free up memory\n",
        "\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgyIgEn1T3Te",
        "outputId": "aeb0dce7-46a3-4e85-9c62-61f2c370a0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 12,288\n",
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
            "number of parameters: 123.59M\n",
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 25, with 19,200 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop\n",
        "\n",
        "Here is where the magic happens!\n",
        "\n",
        "Before we get straight into training - let's look at our model to obtain some key insights."
      ],
      "metadata": {
        "id": "MFQcSiOYUyBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27KCZIhtuRFc",
        "outputId": "41688787-c72f-42a8-e754-7627148855d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OptimizedModule(\n",
            "  (_orig_mod): GPT(\n",
            "    (transformer): ModuleDict(\n",
            "      (wte): Embedding(50304, 768)\n",
            "      (wpe): Embedding(1024, 768)\n",
            "      (drop): Dropout(p=0.0, inplace=False)\n",
            "      (h): ModuleList(\n",
            "        (0-11): 12 x Block(\n",
            "          (ln_1): LayerNorm()\n",
            "          (attn): CausalSelfAttention(\n",
            "            (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
            "            (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm()\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
            "            (gelu): GELU(approximate='none')\n",
            "            (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln_f): LayerNorm()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice our final layer - the `lm_head` - and how it has `out_features=50304`. This number of output features lines up exactly with our vocabulary! When we're making predictions, we're making predictions about which token (in our vocabulary) should be selected next!"
      ],
      "metadata": {
        "id": "NS1oTMdV-oH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First Batch - What is a Batch?\n",
        "\n",
        "Let's look at our first batch to see what exactly a \"batch\" is in this context."
      ],
      "metadata": {
        "id": "NX778-01U2SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch('train') # fetch the very first batch"
      ],
      "metadata": {
        "id": "Gd6LWL9LU14w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X Shape: {X.shape}, Y Shape: {Y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6cC2iGj-q8i",
        "outputId": "c70193fd-8d26-4664-da03-e382d0b6e13b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Shape: torch.Size([12, 1024]), Y Shape: torch.Size([12, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what our X and Y look like!\n",
        "\n",
        "> NOTE: We'll only look at the last 5 tokens of the last batch to get a sense of what is happening under the hood of the data selection."
      ],
      "metadata": {
        "id": "juIxTM6HVrfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[:][-1][-5:])\n",
        "print(Y[:][-1][-5:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lThD-QrrVqlK",
        "outputId": "1bfcad10-c18c-4047-abba-48012e64ae09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18719,    11,   351,   465, 21752], device='cuda:0')\n",
            "tensor([   11,   351,   465, 21752,    11], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how X and Y are simply shifted by a single index - where Y contains (at every index) the token that *follows* X!\n",
        "\n",
        "So essentially - Y contains the *labels* (or target) for X!\n",
        "\n",
        "Let's see how we can leverage this in our training loop!\n",
        "\n",
        "Before we start training - let's import our decoder so we can see specific text that our model is leveraging!"
      ],
      "metadata": {
        "id": "fWgmihsgWBmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)"
      ],
      "metadata": {
        "id": "bL8uueRjyqL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loop (provided through the NanoGPT repository) has a lot of interesting things going on - but we're going to focus on the specific section of the training loop that relates to the loss and logits."
      ],
      "metadata": {
        "id": "xQXAoLD19w4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model = model.module if ddp else model\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100,\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        ####### LOGITS AND LOSS #######\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            print(f\"Our inputs (truncated to the last 15 tokens in the last batch) were: {X[:][-1][-15:].cpu().numpy()}\")\n",
        "            print(f\"Represented in text that is: {[decode(X[:][-1][-15:].cpu().numpy())]}\")\n",
        "            print(f\"Our targets represented in text were: {[decode(Y[:][-1][-15:].cpu().numpy())]}\")\n",
        "            print(f\"Our logits are in shape: {logits.shape}\")\n",
        "            print(f\"The Vocabulary Size is: {logits.shape[-1]}\")\n",
        "            print(f\"The Sequence Length is: {logits.shape[1]}\")\n",
        "            print(f\"Our loss was calculated as: {loss}\")\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "        X, Y = get_batch('train')\n",
        "        scaler.scale(loss).backward()\n",
        "        ###############################\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5:\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhuKbQE6Upm0",
        "outputId": "9057ee95-203f-4f3c-f4f3-54a035dba07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 11.0040, val loss 10.9976\n",
            "Our inputs (truncated to the last 15 tokens in the last batch) were: [ 9005  4432   528   417    11   198 31056   286  2165   844 18719    11\n",
            "   351   465 21752]\n",
            "Represented in text that is: [' Prince Florizel,\\nSon of Polixenes, with his princess']\n",
            "Our targets represented in text were: [' Florizel,\\nSon of Polixenes, with his princess,']\n",
            "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
            "The Vocabulary Size is: 50304\n",
            "The Sequence Length is: 1024\n",
            "Our loss was calculated as: 10.985529899597168\n",
            "iter 0: time 73410.30ms, mfu -100.00%\n",
            "Our inputs (truncated to the last 15 tokens in the last batch) were: [   25   198 33873  3183    11  1497   351   683     0   198   198    51\n",
            " 38409    25   198]\n",
            "Represented in text that is: [':\\nSoldiers, away with him!\\n\\nTutor:\\n']\n",
            "Our targets represented in text were: ['\\nSoldiers, away with him!\\n\\nTutor:\\nAh']\n",
            "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
            "The Vocabulary Size is: 50304\n",
            "The Sequence Length is: 1024\n",
            "Our loss was calculated as: 11.003973007202148\n",
            "iter 1: time 123.32ms, mfu -100.00%\n",
            "Our inputs (truncated to the last 15 tokens in the last batch) were: [4776  502  510  329  262  198 3157  395  638 1015  287 1951  437  296\n",
            "   13]\n",
            "Represented in text that is: [' score me up for the\\nlyingest knave in Christendom.']\n",
            "Our targets represented in text were: [' me up for the\\nlyingest knave in Christendom. What']\n",
            "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
            "The Vocabulary Size is: 50304\n",
            "The Sequence Length is: 1024\n",
            "Our loss was calculated as: 9.738042831420898\n",
            "iter 2: time 344.95ms, mfu -100.00%\n",
            "Our inputs (truncated to the last 15 tokens in the last batch) were: [ 198   54 1670  290  649 1494 1549   13  198  198 4805 1268 5222   25\n",
            "  198]\n",
            "Represented in text that is: [\"\\nWarm and new kill'd.\\n\\nPRINCE:\\n\"]\n",
            "Our targets represented in text were: [\"Warm and new kill'd.\\n\\nPRINCE:\\nSearch\"]\n",
            "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
            "The Vocabulary Size is: 50304\n",
            "The Sequence Length is: 1024\n",
            "Our loss was calculated as: 9.324609756469727\n",
            "iter 3: time 349.95ms, mfu -100.00%\n",
            "Our inputs (truncated to the last 15 tokens in the last batch) were: [  674 10715    13   198   198 15946   455    25   198  5247   284    11\n",
            " 15967    26   345]\n",
            "Represented in text that is: [' our mystery.\\n\\nProvost:\\nGo to, sir; you']\n",
            "Our targets represented in text were: [' mystery.\\n\\nProvost:\\nGo to, sir; you weigh']\n",
            "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
            "The Vocabulary Size is: 50304\n",
            "The Sequence Length is: 1024\n",
            "Our loss was calculated as: 9.514071464538574\n",
            "iter 4: time 360.88ms, mfu -100.00%\n",
            "Our inputs (truncated to the last 15 tokens in the last batch) were: [  11 4379  705 4246  292  339  326  925  345  284 1207  577   11  198\n",
            " 7120]\n",
            "Represented in text that is: [\", seeing 'twas he that made you to depose,\\nYour\"]\n",
            "Our targets represented in text were: [\" seeing 'twas he that made you to depose,\\nYour oath\"]\n",
            "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
            "The Vocabulary Size is: 50304\n",
            "The Sequence Length is: 1024\n",
            "Our loss was calculated as: 8.965362548828125\n",
            "iter 5: time 351.44ms, mfu 9.58%\n",
            "Our inputs (truncated to the last 15 tokens in the last batch) were: [   43  2885    56 20176 24212    51    25   198   464   661   287   262\n",
            "  4675  3960 43989]\n",
            "Represented in text that is: ['LADY CAPULET:\\nThe people in the street cry Romeo']\n",
            "Our targets represented in text were: ['ADY CAPULET:\\nThe people in the street cry Romeo,']\n",
            "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
            "The Vocabulary Size is: 50304\n",
            "The Sequence Length is: 1024\n",
            "Our loss was calculated as: 8.68301010131836\n",
            "iter 6: time 342.08ms, mfu 9.61%\n",
            "Our inputs (truncated to the last 15 tokens in the last batch) were: [ 2236  1282   284 22363  3725    11   326   198  1639  4145   423  7715\n",
            "  1549   502     0]\n",
            "Represented in text that is: [\" shall come to clearer knowledge, that\\nYou thus have publish'd me!\"]\n",
            "Our targets represented in text were: [\" come to clearer knowledge, that\\nYou thus have publish'd me! Gentle\"]\n",
            "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
            "The Vocabulary Size is: 50304\n",
            "The Sequence Length is: 1024\n",
            "Our loss was calculated as: 8.433755874633789\n",
            "iter 7: time 357.41ms, mfu 9.59%\n",
            "Our inputs (truncated to the last 15 tokens in the last batch) were: [  262  1633   198  1870   307   407  4259  1549   287 27666 29079    11\n",
            "   198    39  2502]\n",
            "Represented in text that is: [\" the air\\nAnd be not fix'd in doom perpetual,\\nHover\"]\n",
            "Our targets represented in text were: [\" air\\nAnd be not fix'd in doom perpetual,\\nHover about\"]\n",
            "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
            "The Vocabulary Size is: 50304\n",
            "The Sequence Length is: 1024\n",
            "Our loss was calculated as: 8.148832321166992\n",
            "iter 8: time 353.52ms, mfu 9.58%\n",
            "Our inputs (truncated to the last 15 tokens in the last batch) were: [ 3025  8557  7739   550    11   198  2484   439  2245   393 26724   502\n",
            "    13  8192   314]\n",
            "Represented in text that is: [' whose spiritual counsel had,\\nShall stop or spur me. Have I']\n",
            "Our targets represented in text were: [' spiritual counsel had,\\nShall stop or spur me. Have I done']\n",
            "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
            "The Vocabulary Size is: 50304\n",
            "The Sequence Length is: 1024\n",
            "Our loss was calculated as: 7.851876735687256\n",
            "iter 9: time 348.70ms, mfu 9.59%\n",
            "Our inputs (truncated to the last 15 tokens in the last batch) were: [  407   257  1573   286  8716    30   198  4366  4467    11 15849    13\n",
            "   198   198    45]\n",
            "Represented in text that is: [' not a word of joy?\\nSome comfort, nurse.\\n\\nN']\n",
            "Our targets represented in text were: [' a word of joy?\\nSome comfort, nurse.\\n\\nNurse']\n",
            "Our logits are in shape: torch.Size([12, 1024, 50304])\n",
            "The Vocabulary Size is: 50304\n",
            "The Sequence Length is: 1024\n",
            "Our loss was calculated as: 7.474444389343262\n",
            "iter 10: time 354.66ms, mfu 9.58%\n"
          ]
        }
      ]
    }
  ]
}